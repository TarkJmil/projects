♡, [3/3/2025 11:55 PM]
1. الفرق بين التعلم تحت الإشراف (Supervised Learning) والتعلم غير الإشرافي (Unsupervised Learning):

التعلم تحت الإشراف (Supervised Learning):

التعريف: في هذا النوع من التعلم، يتم تدريب النموذج باستخدام بيانات مدعومة (أي أن كل مثال في البيانات يحتوي على المدخلات مع التصنيف أو الهدف الصحيح، مثل العلامات أو الفئات).

الأمثلة:

تصنيف البريد الإلكتروني: تصنيف رسائل البريد الإلكتروني إلى "مزعج" أو "غير مزعج".

التنبؤ بالأسعار: استخدام البيانات لتدريب نموذج يتنبأ بأسعار المنازل بناءً على الخصائص مثل الحجم والموقع.


المميزات: النماذج تتعلم من الأمثلة السابقة والنتائج المعروفة لتوقع النتائج المستقبلية.


التعلم غير الإشرافي (Unsupervised Learning):

التعريف: في هذا النوع من التعلم، يتم تدريب النموذج باستخدام بيانات غير مصنفة (لا توجد تسميات أو أهداف معروفة)، ويتم اكتشاف الأنماط أو الهياكل الكامنة في البيانات.

الأمثلة:

التجميع (Clustering): مثل استخدام خوارزميات مثل K-means لتجميع العملاء في مجموعات بناءً على خصائصهم.

التخفيض في الأبعاد (Dimensionality Reduction): مثل خوارزمية PCA لاستخراج أهم الميزات من مجموعة بيانات معقدة.


المميزات: يهدف إلى اكتشاف الأنماط والهيكل في البيانات دون الحاجة إلى تسميات مسبقة.



2. ما هو overfitting؟ كيف يمكن معالجة هذه المشكلة؟

تعريف الـ Overfitting:

يحدث الـ overfitting عندما يتعلم النموذج جيدًا جدًا على بيانات التدريب لدرجة أنه يبدأ في حفظ التفاصيل والضوضاء في تلك البيانات بدلاً من اكتشاف الأنماط العامة. النتيجة هي أن أداء النموذج يتدهور عند التعامل مع بيانات جديدة أو بيانات اختبار.


كيفية المعالجة:

1. استخدام تقنيات الـ Regularization: مثل L1 و L2 لمنع النموذج من التكيف الزائد مع بيانات التدريب.


2. تقليل تعقيد النموذج: مثل تقليل عدد الطبقات أو العقد في الشبكات العصبية.


3. التوقف المبكر (Early Stopping): التوقف عن التدريب عندما يبدأ الأداء على مجموعة التحقق في التدهور.


4. زيادة بيانات التدريب (Data Augmentation): استخدام تقنيات مثل التدوير أو التحويل لتوليد المزيد من الأمثلة من نفس البيانات.


5. استخدام Cross-Validation: للتحقق من أداء النموذج عبر بيانات مختلفة لتقليل فرصة التعلم المفرط.




3. الفرق بين الخوارزميات القائمة على نموذج (Model-based algorithms) والخوارزميات القائمة على مثال (Instance-based algorithms):

الخوارزميات القائمة على نموذج (Model-based algorithms):

التعريف: هذه الخوارزميات تبني نموذجًا عامًّا يتم تدريبه على البيانات ثم يُستخدم لتوقع الفئات أو القيم الجديدة. النموذج يمكن استخدامه مع بيانات غير مرئية بعد التدريب.

الأمثلة:

الانحدار الخطي (Linear Regression).

دعم الآلات الشعاعية (SVM).

الشبكات العصبية (Neural Networks).


المميزات: توفر نموذجًا ثابتًا يتم تخزينه واستخدامه لتوقع النتائج الجديدة.


الخوارزميات القائمة على مثال (Instance-based algorithms):

التعريف: هذه الخوارزميات تقوم بتخزين الأمثلة أو النقاط من البيانات المدربة بشكل مباشر، وتستخدم هذه الأمثلة للمقارنة مع البيانات الجديدة لتصنيفها أو توقعها، ولا تبني نموذجًا معقدًا.

الأمثلة:

K-أقرب الجيران (K-Nearest Neighbors, KNN).


المميزات: النموذج لا يتم تدريبه بشكل تقليدي، بل يعتمد على النقاط المدربة في التدريب لتصنيف أو التنبؤ.



4. كيف يمكن استخدام تقنيات regularization (مثل L1 و L2) لمنع overfitting؟

الـ Regularization: هي تقنيات تستخدم لتحسين قدرة النموذج على التعميم ومنع overfitting، من خلال إضافة مصطلحات جزائية إلى دالة الخسارة التي تؤدي إلى تقليل تعقيد النموذج.

L1 Regularization (Lasso):

التعريف: في الـ L1، تتم إضافة الحد المطلق للوزن إلى دالة الخسارة. هذا يساعد في تقليل بعض الأوزان إلى الصفر، مما يؤدي إلى اختيار ميزات أقل ولكن أكثر أهمية.

كيف يساعد في منع Overfitting: عن طريق فرض تقليل الأوزان غير المهمة، وبالتالي تقليل التعقيد في النموذج.


from tensorflow.keras.regularizers import l1
model.add(Dense(units=64, activation='relu', kernel_regularizer=l1(0.01)))

L2 Regularization (Ridge):

التعريف: في الـ L2، تتم إضافة مربع الأوزان إلى دالة الخسارة. هذا يؤدي إلى تقليل الأوزان الكبيرة ولكن دون إلغاء بعض الأوزان تمامًا.

كيف يساعد في منع Overfitting: عن طريق منع الأوزان من أن تصبح كبيرة جدًا، مما يساعد النموذج على التعميم بشكل أفضل على بيانات جديدة.


from tensorflow.keras.regularizers import l2
model.add(Dense(units=64, activation='relu', kernel_regularizer=l2(0.01)))


الاستفادة من الـ Regularization:

♡, [3/3/2025 11:55 PM]
تقنيات مثل L1 و L2 تجعل النماذج أكثر قدرة على التعميم، وتمنعها من التكيف مع الضوضاء أو الأنماط العرضية في بيانات التدريب. يتم استخدام الـ Regularization عادة في الشبكات العصبية والانحدار الخطي لتعزيز الأداء على بيانات جديدة.