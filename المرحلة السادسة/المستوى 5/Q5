1. كيفية التعامل مع القيم المفقودة (Missing Values) في مجموعة بيانات؟

يمكن التعامل مع القيم المفقودة بعدة طرق، أهمها:

إزالة الصفوف أو الأعمدة: إذا كانت القيم المفقودة تشكل جزءًا ضئيلًا من البيانات، يمكن إزالة الصفوف أو الأعمدة التي تحتوي على هذه القيم.

ملء القيم المفقودة: يمكن استبدال القيم المفقودة باستخدام وسائل مثل:

القيمة المتوسطة أو الوسيط (Mean/Median): يتم ملء القيم المفقودة باستخدام المتوسط أو الوسيط للعمود.

القيمة الأكثر تكرارًا (Mode): يتم استبدال القيم المفقودة باستخدام القيمة الأكثر تكرارًا (في حال كانت البيانات فئوية).

الملء باستخدام النماذج: يمكن استخدام تقنيات مثل الانحدار أو الخوارزميات الأخرى للتنبؤ بالقيم المفقودة.


إبقائها كما هي: في بعض الحالات، قد يُفضل إبقاء القيم المفقودة في البيانات، خاصة في الخوارزميات التي يمكنها التعامل معها بشكل طبيعي مثل الأشجار العشوائية.


2. شرح كيفية استخدام التطبيع (Normalization) و التقييس (Standardization) للبيانات قبل تدريب النموذج؟

التطبيع (Normalization): يتم تحويل البيانات إلى نطاق يتراوح بين 0 و 1. هذا مفيد عندما يكون لدينا بيانات بمقاييس مختلفة وتحتاج إلى جعل كل الخاصيات في نفس النطاق. يمكن استخدام قاعدة Min-Max Normalization لهذه العملية:


X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}

التقييس (Standardization): في هذه الطريقة، يتم تحويل البيانات بحيث يكون لها متوسط صفر وانحراف معياري 1. تُستخدم عندما تكون البيانات تحتوي على توزيعات غير متساوية أو ذات تباين كبير. يتم استخدام القاعدة التالية:


X_{std} = \frac{X - \mu}{\sigma}

الفرق بينهما: يتم استخدام التطبيع عندما تكون البيانات تقع في نطاق معين وتحتاج إلى تمثيلها بشكل موحد (مثلاً، في الشبكات العصبية). أما التقييس فيتم استخدامه بشكل أكثر في تقنيات مثل الانحدار الخطي أو تحليل المكونات الرئيسية (PCA) حيث تكون الفروق في الانحرافات المعيارية بين المتغيرات أكثر أهمية.

3. ما هو الاختيار الأمثل للخصائص (Feature Selection) وكيف يمكن تطبيقه على مجموعة بيانات كبيرة؟

الاختيار الأمثل للخصائص هو عملية تحديد وتحديد الخصائص (المتغيرات) الأكثر تأثيرًا في التنبؤ بالنتيجة. يمكن استخدام عدة تقنيات لاختيار الخصائص:

التصفية (Filter methods): مثل استخدام تقنيات إحصائية (مثل اختبار Chi-Square أو اختبار التباين) لاختيار الخصائص الأكثر ارتباطًا بالهدف.

التغليف (Wrapper methods): مثل الخوارزميات التي تعتمد على اختيار مجموعة من الخصائص بناءً على الأداء، مثل خوارزميات التنقيب الأمامي (Forward Selection) أو التنقيب العكسي (Backward Elimination).

التضمين (Embedded methods): مثل خوارزميات مثل شجرة القرار أو الانحدار اللوجستي التي تتضمن اختيار الخصائص أثناء عملية تدريب النموذج.


لتطبيق هذه العمليات على مجموعة بيانات كبيرة:

استخدام تقنيات التعلم الآلي مثل الأشجار العشوائية أو XGBoost لاختيار الخصائص الأكثر أهمية.

تقنيات تقليل الأبعاد مثل تحليل المكونات الرئيسية (PCA) يمكن استخدامها لتقليل عدد الخصائص مع الحفاظ على أكبر قدر ممكن من المعلومات.

يمكن أيضًا استخدام التخزين المؤقت لتخزين النتائج الوسيطة أثناء المعالجة لضمان الكفاءة في الوقت والمكان.


4. كيف يمكن التعامل مع البيانات غير المتوازنة (Imbalanced Data) في مسائل التصنيف؟

البيانات غير المتوازنة هي حالة حيث تكون الفئات في بيانات التصنيف غير موزعة بشكل متساوٍ. لتعامل مع هذه المشكلة، يمكن استخدام بعض الأساليب مثل:

إعادة التوزيع (Resampling):

زيادة البيانات في الفئة النادرة (Oversampling): يمكن تكرار العينات من الفئة الأقل تمثيلاً.

تقليل البيانات في الفئة الأكثر تكرارًا (Undersampling): تقليل العينات من الفئة الأكثر تمثيلاً.


الوزن العكسي (Class Weighting): إعطاء وزن أكبر للفئة النادرة في الخوارزميات التي تدعم ذلك، مثل الانحدار اللوجستي أو الأشجار العشوائية.

استخدام خوارزميات متخصصة: مثل SMOTE (Synthetic Minority Over-sampling Technique)، التي تُستخدم لإنشاء عينات اصطناعية من الفئة الأقل تمثيلًا.

استخدام معايير تقييم ملائمة: مثل الدقة المعدلة (Balanced Accuracy) أو مصفوفة الالتباس و الـ F1-Score بدلاً من الدقة التقليدية، لأن الدقة قد تكون مضللة في حالات البيانات غير المتوازنة.